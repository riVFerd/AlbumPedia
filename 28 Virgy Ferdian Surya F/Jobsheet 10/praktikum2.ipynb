{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "initial_id",
      "metadata": {
        "collapsed": true,
        "id": "initial_id"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
      ],
      "metadata": {
        "id": "Grc_Rly56Z4-",
        "outputId": "80fa0707-651b-4816-83a8-22d2f3eef58e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "Grc_Rly56Z4-",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
            "1115394/1115394 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Read, then decode for py2 compat.\n",
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "# length of text is the number of characters in it\n",
        "print(f'Length of text: {len(text)} characters')"
      ],
      "metadata": {
        "id": "yDIGKUyp5-qL",
        "outputId": "8bde5e54-507c-493e-9a55-8d40d9f2eca5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "yDIGKUyp5-qL",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of text: 1115394 characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Take a look at the first 250 characters in text\n",
        "print(text[:250])"
      ],
      "metadata": {
        "id": "nPbft41q6BIi",
        "outputId": "3442ce0f-0123-4544-dbac-6c3ef1782da3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "nPbft41q6BIi",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The unique characters in the file\n",
        "vocab = sorted(set(text))\n",
        "print(f'{len(vocab)} unique characters')"
      ],
      "metadata": {
        "id": "Jy7oXnTC6Dz3",
        "outputId": "2495aaa8-37a0-431f-829d-1e078e99b0ce",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "Jy7oXnTC6Dz3",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "65 unique characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "example_texts = ['abcdefg', 'xyz']\n",
        "chars = tf.strings.unicode_split(example_texts, input_encoding='UTF-8')\n",
        "chars"
      ],
      "metadata": {
        "id": "uhMHYseU6llZ",
        "outputId": "892ed32f-d455-485f-a1d0-dee009732328",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "uhMHYseU6llZ",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ids_from_chars = tf.keras.layers.StringLookup(\n",
        "vocabulary=list(vocab), mask_token=None)"
      ],
      "metadata": {
        "id": "mGeOELs263iX"
      },
      "id": "mGeOELs263iX",
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ids = ids_from_chars(chars)\n",
        "ids"
      ],
      "metadata": {
        "id": "khYMXilO6qCK",
        "outputId": "7f295fc5-e939-40e8-a5bd-c033454722d1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "khYMXilO6qCK",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[40, 41, 42, 43, 44, 45, 46], [63, 64, 65]]>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chars_from_ids = tf.keras.layers.StringLookup(\n",
        "    vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None)"
      ],
      "metadata": {
        "id": "7gOaortn6uCk"
      },
      "id": "7gOaortn6uCk",
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chars = chars_from_ids(ids)\n",
        "chars"
      ],
      "metadata": {
        "id": "08Zs1Dfj7Lbe",
        "outputId": "d4e7f02e-f137-4c1a-9e99-cb395fe3111c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "08Zs1Dfj7Lbe",
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.strings.reduce_join(chars, axis=-1).numpy()"
      ],
      "metadata": {
        "id": "j_4-qfPJ7NKL",
        "outputId": "de14941d-0f85-4f06-e9a7-6733c3516c0f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "j_4-qfPJ7NKL",
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([b'abcdefg', b'xyz'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def text_from_ids(ids):\n",
        "    return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)"
      ],
      "metadata": {
        "id": "wSIWFG7i7T5a"
      },
      "id": "wSIWFG7i7T5a",
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))\n",
        "all_ids"
      ],
      "metadata": {
        "id": "czMzsivp7WTD",
        "outputId": "5672f323-acee-4db9-9df3-2aca24ce4fc1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "czMzsivp7WTD",
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1115394,), dtype=int64, numpy=array([19, 48, 57, ..., 46,  9,  1])>"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)"
      ],
      "metadata": {
        "id": "p7l8UPac7eOo"
      },
      "id": "p7l8UPac7eOo",
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for ids in ids_dataset.take(10):\n",
        "    print(chars_from_ids(ids).numpy().decode('utf-8'))"
      ],
      "metadata": {
        "id": "upYofCXa7hPZ",
        "outputId": "e9223aeb-bd24-453e-8398-475ae2c430c7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "upYofCXa7hPZ",
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F\n",
            "i\n",
            "r\n",
            "s\n",
            "t\n",
            " \n",
            "C\n",
            "i\n",
            "t\n",
            "i\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seq_length = 100"
      ],
      "metadata": {
        "id": "Xaa6bkV27jzy"
      },
      "id": "Xaa6bkV27jzy",
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "\n",
        "for seq in sequences.take(1):\n",
        "  print(chars_from_ids(seq))"
      ],
      "metadata": {
        "id": "ItrrH9Jj7mqA",
        "outputId": "9d74f3a2-e1e5-4582-af33-66853a05127f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "ItrrH9Jj7mqA",
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[b'F' b'i' b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':'\n",
            " b'\\n' b'B' b'e' b'f' b'o' b'r' b'e' b' ' b'w' b'e' b' ' b'p' b'r' b'o'\n",
            " b'c' b'e' b'e' b'd' b' ' b'a' b'n' b'y' b' ' b'f' b'u' b'r' b't' b'h'\n",
            " b'e' b'r' b',' b' ' b'h' b'e' b'a' b'r' b' ' b'm' b'e' b' ' b's' b'p'\n",
            " b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'A' b'l' b'l' b':' b'\\n' b'S' b'p' b'e'\n",
            " b'a' b'k' b',' b' ' b's' b'p' b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'F' b'i'\n",
            " b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':' b'\\n' b'Y'\n",
            " b'o' b'u' b' '], shape=(101,), dtype=string)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for seq in sequences.take(5):\n",
        "    print(text_from_ids(seq).numpy())"
      ],
      "metadata": {
        "id": "lXXYwhmQ7oTQ",
        "outputId": "9d19e0d6-2df6-455f-d267-fb008b7c0c31",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "lXXYwhmQ7oTQ",
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n",
            "b'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k'\n",
            "b\"now Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us ki\"\n",
            "b\"ll him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be d\"\n",
            "b'one: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citi'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def split_input_target(sequence):\n",
        " input_text = sequence[:-1]\n",
        " target_text = sequence[1:]\n",
        " return input_text, target_text"
      ],
      "metadata": {
        "id": "KAN3x4Cn7qKB"
      },
      "id": "KAN3x4Cn7qKB",
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "split_input_target(list(\"Tensorflow\"))"
      ],
      "metadata": {
        "id": "qUSKEUsI7yLU",
        "outputId": "b2a022f2-7b56-4af8-eec8-9855fac64d35",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "qUSKEUsI7yLU",
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['T', 'e', 'n', 's', 'o', 'r', 'f', 'l', 'o'],\n",
              " ['e', 'n', 's', 'o', 'r', 'f', 'l', 'o', 'w'])"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = sequences.map(split_input_target)"
      ],
      "metadata": {
        "id": "4BaOuN6D71Dh"
      },
      "id": "4BaOuN6D71Dh",
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for input_example, target_example in dataset.take(1):\n",
        " print(\"Input :\", text_from_ids(input_example).numpy())\n",
        " print(\"Target:\", text_from_ids(target_example).numpy())"
      ],
      "metadata": {
        "id": "PSVjaCvf73oY",
        "outputId": "8880f23d-9814-452b-8648-a8425c4ee267",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "PSVjaCvf73oY",
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input : b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n",
            "Target: b'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Batch size\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Buffer size to shuffle the dataset\n",
        "# (TF data is designed to work with possibly infinite sequences,\n",
        "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
        "# it maintains a buffer in which it shuffles elements).\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = (\n",
        "    dataset\n",
        "    .shuffle(BUFFER_SIZE)\n",
        "    .batch(BATCH_SIZE, drop_remainder=True)\n",
        "    .prefetch(tf.data.experimental.AUTOTUNE))\n",
        "\n",
        "dataset"
      ],
      "metadata": {
        "id": "CCHJW0ma76Xv",
        "outputId": "3b8d1c22-fde0-45f6-97fa-60246d640f7d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "CCHJW0ma76Xv",
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<_PrefetchDataset element_spec=(TensorSpec(shape=(64, 100), dtype=tf.int64, name=None), TensorSpec(shape=(64, 100), dtype=tf.int64, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Length of the vocabulary in StringLookup Layer\n",
        "vocab_size = len(ids_from_chars.get_vocabulary())\n",
        "\n",
        "# The embedding dimension\n",
        "embedding_dim = 256\n",
        "\n",
        "# Number of RNN units\n",
        "rnn_units = 1024"
      ],
      "metadata": {
        "id": "PMi2CWmr7_pF"
      },
      "id": "PMi2CWmr7_pF",
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MyModel(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
        "    super().__init__(self)\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(rnn_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True)\n",
        "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "  def call(self, inputs, states=None, return_state=False, training=False):\n",
        "    x = inputs\n",
        "    x = self.embedding(x, training=training)\n",
        "    if states is None:\n",
        "      states = self.gru.get_initial_state(x)\n",
        "    x, states = self.gru(x, initial_state=states, training=training)\n",
        "    x = self.dense(x, training=training)\n",
        "\n",
        "    if return_state:\n",
        "      return x, states\n",
        "    else:\n",
        "      return x"
      ],
      "metadata": {
        "id": "27Eo5fKf8BzT"
      },
      "id": "27Eo5fKf8BzT",
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = MyModel(\n",
        "    vocab_size=vocab_size,\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units)"
      ],
      "metadata": {
        "id": "nkVy21Ll8Dqp"
      },
      "id": "nkVy21Ll8Dqp",
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "    example_batch_predictions = model(input_example_batch)\n",
        "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
      ],
      "metadata": {
        "id": "y9WpSgJN8E7e",
        "outputId": "25647d00-0442-46af-fd9c-9f9cb34c9a18",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "y9WpSgJN8E7e",
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 100, 66) # (batch_size, sequence_length, vocab_size)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "Pl1nu1UV8GW0",
        "outputId": "d0e0f333-ad48-4262-ab64-fb7aa5371d4e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "Pl1nu1UV8GW0",
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"my_model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       multiple                  16896     \n",
            "                                                                 \n",
            " gru (GRU)                   multiple                  3938304   \n",
            "                                                                 \n",
            " dense (Dense)               multiple                  67650     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4022850 (15.35 MB)\n",
            "Trainable params: 4022850 (15.35 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
        "sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()"
      ],
      "metadata": {
        "id": "MmuMTDH38HXn"
      },
      "id": "MmuMTDH38HXn",
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sampled_indices"
      ],
      "metadata": {
        "id": "OFfCkVP38Ikv",
        "outputId": "7fdc6a73-3d00-45f5-d6cb-81d52d0cb008",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "OFfCkVP38Ikv",
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([42, 16, 21, 37, 57, 23, 65,  2, 56, 64, 26,  2, 59,  0, 36, 53, 28,\n",
              "       62, 43, 28, 57, 50, 63,  5, 12,  8, 20,  6, 40, 57, 59,  1, 33, 19,\n",
              "       17, 25,  4, 50, 27, 29, 61, 13, 33,  7,  3, 24, 23, 28, 53, 35, 28,\n",
              "        0, 12,  4, 57, 10, 33, 60, 32, 47, 14, 26, 38, 65, 21, 61, 36, 43,\n",
              "       42, 22, 54, 41, 21, 49, 24, 48, 21, 14, 55, 13, 13, 27, 26,  7, 34,\n",
              "       39, 23,  9, 29,  1, 30, 31, 50,  7, 45, 21,  1, 59, 62, 17])"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Input:\\n\", text_from_ids(input_example_batch[0]).numpy())\n",
        "print()\n",
        "print(\"Next Char Predictions:\\n\", text_from_ids(sampled_indices).numpy())"
      ],
      "metadata": {
        "id": "k4puX63t8NDS",
        "outputId": "bc1e2829-e870-401c-abeb-06de6b6f90b3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "k4puX63t8NDS",
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input:\n",
            " b'horsed\\nWith variable complexions, all agreeing\\nIn earnestness to see him: seld-shown flamens\\nDo pres'\n",
            "\n",
            "Next Char Predictions:\n",
            " b\"cCHXrJz qyM t[UNK]WnOwdOrkx&;-G'art\\nTFDL$kNPv?T,!KJOnVO[UNK];$r3TuShAMYzHvWdcIobHjKiHAp??NM,UZJ.P\\nQRk,fH\\ntwD\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)"
      ],
      "metadata": {
        "id": "zG-6GVfi8P3W"
      },
      "id": "zG-6GVfi8P3W",
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_batch_mean_loss = loss(target_example_batch, example_batch_predictions)\n",
        "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
        "print(\"Mean loss:        \", example_batch_mean_loss)"
      ],
      "metadata": {
        "id": "nJEl7a-z8TXE",
        "outputId": "4b6396cf-9ba6-4937-854f-c1d2a3dbda6f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "nJEl7a-z8TXE",
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction shape:  (64, 100, 66)  # (batch_size, sequence_length, vocab_size)\n",
            "Mean loss:         tf.Tensor(4.189288, shape=(), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.exp(example_batch_mean_loss).numpy()"
      ],
      "metadata": {
        "id": "Nmo3HeYT8Wg0",
        "outputId": "469e7d42-e456-4897-b0c9-4f515cd2d940",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "Nmo3HeYT8Wg0",
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "65.975815"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam', loss=loss)"
      ],
      "metadata": {
        "id": "sxDMjeFe8X0s"
      },
      "id": "sxDMjeFe8X0s",
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)"
      ],
      "metadata": {
        "id": "q1a0hIUP8Y69"
      },
      "id": "q1a0hIUP8Y69",
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 20"
      ],
      "metadata": {
        "id": "MHBNUruS8djv"
      },
      "id": "MHBNUruS8djv",
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
      ],
      "metadata": {
        "id": "6mX5jrV88hvN",
        "outputId": "88f79841-6b4f-46a6-e268-f1ebe9093c59",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "6mX5jrV88hvN",
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "172/172 [==============================] - 16s 61ms/step - loss: 2.7073\n",
            "Epoch 2/20\n",
            "172/172 [==============================] - 14s 61ms/step - loss: 1.9739\n",
            "Epoch 3/20\n",
            "172/172 [==============================] - 13s 62ms/step - loss: 1.6948\n",
            "Epoch 4/20\n",
            "172/172 [==============================] - 13s 61ms/step - loss: 1.5370\n",
            "Epoch 5/20\n",
            "172/172 [==============================] - 13s 62ms/step - loss: 1.4400\n",
            "Epoch 6/20\n",
            "172/172 [==============================] - 12s 59ms/step - loss: 1.3741\n",
            "Epoch 7/20\n",
            "172/172 [==============================] - 12s 58ms/step - loss: 1.3211\n",
            "Epoch 8/20\n",
            "172/172 [==============================] - 12s 58ms/step - loss: 1.2768\n",
            "Epoch 9/20\n",
            "172/172 [==============================] - 12s 59ms/step - loss: 1.2348\n",
            "Epoch 10/20\n",
            "172/172 [==============================] - 12s 59ms/step - loss: 1.1951\n",
            "Epoch 11/20\n",
            "172/172 [==============================] - 11s 58ms/step - loss: 1.1549\n",
            "Epoch 12/20\n",
            "172/172 [==============================] - 12s 58ms/step - loss: 1.1133\n",
            "Epoch 13/20\n",
            "172/172 [==============================] - 12s 59ms/step - loss: 1.0697\n",
            "Epoch 14/20\n",
            "172/172 [==============================] - 12s 58ms/step - loss: 1.0242\n",
            "Epoch 15/20\n",
            "172/172 [==============================] - 12s 58ms/step - loss: 0.9761\n",
            "Epoch 16/20\n",
            "172/172 [==============================] - 12s 58ms/step - loss: 0.9259\n",
            "Epoch 17/20\n",
            "172/172 [==============================] - 12s 58ms/step - loss: 0.8742\n",
            "Epoch 18/20\n",
            "172/172 [==============================] - 12s 60ms/step - loss: 0.8233\n",
            "Epoch 19/20\n",
            "172/172 [==============================] - 12s 60ms/step - loss: 0.7734\n",
            "Epoch 20/20\n",
            "172/172 [==============================] - 12s 60ms/step - loss: 0.7263\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class OneStep(tf.keras.Model):\n",
        "  def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
        "    super().__init__()\n",
        "    self.temperature = temperature\n",
        "    self.model = model\n",
        "    self.chars_from_ids = chars_from_ids\n",
        "    self.ids_from_chars = ids_from_chars\n",
        "\n",
        "    # Create a mask to prevent \"[UNK]\" from being generated.\n",
        "    skip_ids = self.ids_from_chars(['[UNK]'])[:, None]\n",
        "    sparse_mask = tf.SparseTensor(\n",
        "        # Put a -inf at each bad index.\n",
        "        values=[-float('inf')]*len(skip_ids),\n",
        "        indices=skip_ids,\n",
        "        # Match the shape to the vocabulary\n",
        "        dense_shape=[len(ids_from_chars.get_vocabulary())])\n",
        "    self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
        "\n",
        "  @tf.function\n",
        "  def generate_one_step(self, inputs, states=None):\n",
        "    # Convert strings to token IDs.\n",
        "    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
        "    input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
        "\n",
        "    # Run the model.\n",
        "    # predicted_logits.shape is [batch, char, next_char_logits]\n",
        "    predicted_logits, states = self.model(inputs=input_ids, states=states,\n",
        "                                          return_state=True)\n",
        "    # Only use the last prediction.\n",
        "    predicted_logits = predicted_logits[:, -1, :]\n",
        "    predicted_logits = predicted_logits/self.temperature\n",
        "    # Apply the prediction mask: prevent \"[UNK]\" from being generated.\n",
        "    predicted_logits = predicted_logits + self.prediction_mask\n",
        "\n",
        "    # Sample the output logits to generate token IDs.\n",
        "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
        "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
        "\n",
        "    # Convert from token ids to characters\n",
        "    predicted_chars = self.chars_from_ids(predicted_ids)\n",
        "\n",
        "    # Return the characters and model state.\n",
        "    return predicted_chars, states"
      ],
      "metadata": {
        "id": "puz0Bo_z8k8L"
      },
      "id": "puz0Bo_z8k8L",
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)"
      ],
      "metadata": {
        "id": "vvHR9lZQ8puL"
      },
      "id": "vvHR9lZQ8puL",
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant(['ROMEO:'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n",
        "print('\\nRun time:', end - start)"
      ],
      "metadata": {
        "id": "LikVOLus8tMp",
        "outputId": "a12e0b99-ab96-4c52-a577-2f232fc4289a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "LikVOLus8tMp",
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROMEO:\n",
            "Ah, cousin, I, thou dread'st upon the Tower.\n",
            "\n",
            "KING RICHARD II:\n",
            "Master?\n",
            "\n",
            "MENENIUS:\n",
            "There's a combany in the common mouth. I am honest,\n",
            "Fellow, you must just anither so petruchio's wild;\n",
            "Hood my servant, here that is no wigning\n",
            "Thom when you will from mercy and in alliance\n",
            "Be safely affairs to be; and in heaven\n",
            "That thought therein I should, and but at home too hear,\n",
            "The conceit and take the hear of urged to one,\n",
            "Yet to revenge upon the thrower. Pray, Magar\n",
            "May handle to encounter it.\n",
            "\n",
            "NORTHUMBERLAND:\n",
            "No more than what is think, what you mean no more\n",
            "Than when your party cruples for the conqueror.\n",
            "Have you stand loved the seyness when a most royal king\n",
            "Snave's herm? Dust of we had been a better-face\n",
            "Uppears of peaceings thousand jocks,\n",
            "The weakness where he is excellent.\n",
            "I'll lean us not; for, were it as I love my\n",
            "father is comin for means to do.\n",
            "\n",
            "KING HENRY VI:\n",
            "An dost thou place I have loved the time\n",
            "May in his son, who wanders provoked him,\n",
            "As he in that to hear a loader of his\n",
            "open. \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 3.212812900543213\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant(['ROMEO:', 'ROMEO:', 'ROMEO:', 'ROMEO:', 'ROMEO:'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result, '\\n\\n' + '_'*80)\n",
        "print('\\nRun time:', end - start)"
      ],
      "metadata": {
        "id": "oEIJdk2b8wMR",
        "outputId": "fb8a0ac9-d393-4134-d1ca-ae65d13aba04",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "oEIJdk2b8wMR",
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[b\"ROMEO:\\nThat's settled souls; therefore, farewell.\\n\\nGREEN:\\nBecou, the instant arms thus expedient laugheance;\\nAnd more, with daughter, wroughts and witch\\nBelow'd upon you; and of them are not\\nUntil the thrustand nobles of a treacherous son,\\nForbid, so though some spring-time past of ever,\\nHear me receive, as 'tis thus I'll carry me\\nTo think I in a Clarence closed with melancholy,\\nAnd do them down. I prithee, goodly save the measure\\ngiven to any child hence; and therefore, fain,\\nGood earning in nothing but bosom. Lay her mine,\\nmake a less of the house of Lancaster; Talkin,\\nI am the last tale to our personalth,\\nAnd driven into the manice of our prideal,\\nHas held as missobser'd reverence;\\nBut whom hercens impossibil,\\nI prithee, 'pardon' 'sabel, and say no help,\\nThe gates of yon and one ilonce! and\\nthen let he give him to-night seek to tell me hand it.\\nA whoreform to go with thee!\\n\\nFLORIZEL:\\nWe must,\\nAnd thus play we hear so to get Romeo?\\n\\nROMEO:\\nAt lish-famous repost, that he is a man,\\nFor one p\"\n",
            " b\"ROMEO:\\nThis is a wary with the towns; and therefore, if\\nstand, I were a falcon od too; God forbid\\nYour Rome impression like a cupit,\\nYet knowing bown: me here upon thy garden--lets,\\nO--you bid fall, two daughters; then turn twenty postrussion;\\nAnd there the third in stern; the shepherd say\\nNot therefore dost thou lost my it.\\n\\nDUKE VINCENTIO:\\nIt was a fun of them.\\n\\nBIONDELLO:\\nHe hath been more!\\n\\nFirst Servant:\\nMadam, unable thanks.\\n\\nSLY:\\nI talk not for his servant.\\n\\nLADY ANNE:\\nWe two view of this custom, if I see\\nMight will fly to deport this fault a suitor?\\n\\nThird Servingman:\\nI darely brake frame.\\n\\nVALERIA:\\nIn this time, 'god patience, fellow, get thee hence.\\n\\nBALOHBO:\\nAnd burn it now; shall's good heart Keep on here!\\nIf I would yield him then?\\n\\nBISHOP OF CAMTLY:\\nAm I left me in her hand:\\nAway with him!\\n\\nBUSHY:\\nMarry, that in command that name in prison; aside,\\nThe primest spirit in the extremest\\nThe head, most wind and me. Lord Rivers, Valubaur?\\nNay, you were lodged, even suitors here the g\"\n",
            " b\"ROMEO:\\nGo tog, It is it heels after a weeping book:\\nI am not foul insolent till they were,\\nWhen weeping of a dangerous that.\\n\\nBUCKINGHAM:\\nWell, briefly, we have ever ever go\\nwell you will hand thus; and in thy offence wrough\\nAll accetted aid that thou dost in\\nthe king;\\nMore cried to one whose blocked days of Adward's grace.\\n\\nEXETER:\\nMy Lord of, Duron, Juliet,\\nSee you were better deadly long I say\\n'sickling his folly; I'll not be done.\\n\\nJOHN OF GAUNT:\\nThese lies in one, let them duke's winds,\\nAs cleas of person, noble and both beggary\\nDeserved that had my punitating with thy\\nblest. There's for my poor house, from thee, here\\nPass you some strange fellow! wept, hark!\\n\\nKING RICHARD II:\\nWe did not be much offence with this wonder.\\nA jealous hood, and a tops thou art extraught,\\nTo muse and seldom that any infant, sir.\\n\\nABRAHAM:\\nDoth yond another, when?\\n\\nPOMPEY:\\nI'll follow him, and here I will be crossed:\\nWhere be a heavy son-seem to delp and set:\\nYet that thy love crave them for thee.\\n\\nLORD ROSS:\"\n",
            " b\"ROMEO:\\nThey shall go walk again: therefore I'll aid\\nTo weed the gown of York; and thou art to\\nBe king: my mother, will you go with me?\\nBut art thou, and all rest under gale\\nTheir cause as free instectable strong?\\nNow, Mordon, man; good counsel, lords.\\nMy Lord of Which a dishal thrust do thee spill;\\nAnd there's no dishonour body is,\\nAnd triumphs in his ornaments: then sweet\\nWise George and false favour's reverence,\\nOr womanish in bands that he's not to beg,\\nfrom either scale, the other is her match.\\n\\nGREEN:\\nMy lord\\nIf deaply be shedly pray, where being old\\nThat seems unkind that it occaping, when he would\\nnot go and of its: there am I stood to't\\nBoland Clifford and Lady Bona, my most reverend\\nFrom Frompt and claps! Revenue you go, no doubt;\\nA woman of the hoom: these corood prove mistress;\\nSo should I do it any nobly entertain\\nMay hull destroy: then we thank giddy foot,\\nDoth churchrickly but to-morrow, if he\\nshall know you to Saint Lukin's own of our amazed:\\nHath she too rest on men, he shoul\"\n",
            " b\"ROMEO:\\nGood morrow, neighbour,' and nothing can, do:\\nAnd will revenge, so guiltyly as in a king,\\nAnd ball deliceing bear a sword of reign.\\n\\nSecond Lord:\\nBid this hear some never of his power?\\n\\nFRIAR LAURENCE:\\nRenowned for that senner; though I think it meet himself,\\nNor with the colour of Alriow's brother,\\nAfter the rebel queen, awaked young Romeo\\n\\nTRANIO:\\nAnd, what a jastice was not made before.\\n\\nKING HENRY VI:\\nAnd say you see, there's spake it home:\\nAnd yet give God of kings, this is he think,\\nYou up and undertake the degrer,\\nHowing at him and alive, Thou'rt cross.\\nI am not true; you may a deedless violence!\\nThou'ldst thrust to wake thee return from thence,\\nTo run upon the world. Fetch on the ground,\\nNor by the trach work; for I will perforce\\nBlowning, what thou hast nor young Frizeth.\\nLet them had.\\n\\nMENENIUS:\\nPray you, there doth make power\\nSeen thou must unberint that skin, and that the state\\nTo more have embraced their fruit?\\n\\nBUCKINGHAM:\\nFor what, I'll swear 'twas not hanging.\\n\\nLUCIO:\\n\"], shape=(5,), dtype=string) \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 3.5168981552124023\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.saved_model.save(one_step_model, 'one_step')\n",
        "one_step_reloaded = tf.saved_model.load('one_step')"
      ],
      "metadata": {
        "id": "sOZr6sQt8xs8",
        "outputId": "14cfc240-ba58-4a21-c6be-c0c535b511ba",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "sOZr6sQt8xs8",
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Skipping full serialization of Keras layer <__main__.OneStep object at 0x782c25575ed0>, because it is not built.\n",
            "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n",
            "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "states = None\n",
        "next_char = tf.constant(['ROMEO:'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(100):\n",
        "  next_char, states = one_step_reloaded.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "print(tf.strings.join(result)[0].numpy().decode(\"utf-8\"))"
      ],
      "metadata": {
        "id": "HwqZq1YO8zgR",
        "outputId": "3ef6c993-7717-4d5c-9137-aa5398323ffa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "HwqZq1YO8zgR",
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROMEO:\n",
            "The swine of his appetite, how at your voices,\n",
            "If not by needful over love,\n",
            "Delay the corrupt our t\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LR6Fv7RT800z"
      },
      "id": "LR6Fv7RT800z",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}